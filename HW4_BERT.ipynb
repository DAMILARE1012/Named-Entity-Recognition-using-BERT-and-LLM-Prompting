{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJIAQTsAzkZ0"
   },
   "source": [
    "# HW4: Fine-tuning BERT for entity labeling\n",
    "This notebook contains starter code for finetuning a BERT-style model for the task of entity recognition. It has minimal text so you can easily copy it to **handin.py** when you submit.  Please read all the comments in the code as they contain important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 12 08:43:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.58.02              Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install torchinfo seaborn numpy pandas transformers matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UjihAVr90bDo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: datasets in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: evaluate in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: dill in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (0.25.2)\n",
      "Requirement already satisfied: packaging in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: seqeval in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from seqeval) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/damilare1012/miniconda3/envs/NLP_Assignments/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# This code block just contains standard setup code for running in Python\n",
    "import time\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset #random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix the random seed(s) for reproducability\n",
    "torch.random.manual_seed(8942764)\n",
    "torch.cuda.manual_seed(8942764)\n",
    "np.random.seed(8942764)\n",
    "\n",
    "# Please set your device by uncommenting the right version below\n",
    "\n",
    "# On Colab or on a machine with access to an Nvidia GPU use the following setting\n",
    "# device = 'cuda:1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# if you have an Apple Silicon machine with a GPU, use the following setting\n",
    "# this should about 3-4 times faster that running it on just CPU\n",
    "# device = 'mps'\n",
    "\n",
    "# If you will use a cpu, this is the setting\n",
    "# device = 'cpu'\n",
    "\n",
    "# Note that in handin.py these next two lines will need to be removed\n",
    "# if you are going run this on your personal machine you will need to install\n",
    "# these locally in the shell/terminal.\n",
    "\n",
    "# !pip install protobuf==3.20.2\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install seqeval\n",
    "\n",
    "from transformers import AutoTokenizer, BertModel, DataCollatorForTokenClassification\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm if GPU is available...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "Number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "duHZ1XZMoYkZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['I-Aquatic_animal', 'B-Deity', 'B-Mythological_king', 'I-Mythological_king', 'I-Cretaceous_dinosaur', 'B-Aquatic_animal', 'B-Aquatic_mammal', 'I-Goddess', 'I-Deity', 'B-Cretaceous_dinosaur', 'I-Aquatic_mammal', 'B-Goddess', 'O']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['para_index', 'title', 'doc_id', 'content', 'page_id', 'id', 'tokens', 'ner_strings', 'ner_tags'],\n",
      "        num_rows: 1749\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['para_index', 'title', 'doc_id', 'content', 'page_id', 'id', 'tokens', 'ner_strings', 'ner_tags'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['para_index', 'title', 'doc_id', 'content', 'page_id', 'id', 'tokens', 'ner_strings', 'ner_tags'],\n",
      "        num_rows: 303\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import ClassLabel, Sequence, load_dataset\n",
    "\n",
    "# Load the dataset from JSON files for train, dev, and test splits\n",
    "data_splits = load_dataset('json', data_files={'train': 'dinos_and_deities_train_bio.jsonl', 'dev': 'dinos_and_deities_dev_bio_sm.jsonl', 'test': 'dinos_and_deities_test_bio_nolabels.jsonl'})\n",
    "\n",
    "# Define the file name containing the label names\n",
    "label_names_fname = \"dinos_and_deities_train_bio.jsonl.labels\"\n",
    "\n",
    "# Initialize a list to store the label names\n",
    "labels_int2str = []\n",
    "\n",
    "# Read the label names from the file and split them into a list\n",
    "with open(label_names_fname) as f:\n",
    "    labels_int2str = f.read().split()\n",
    "\n",
    "# Print the label names\n",
    "print(f\"Labels: {labels_int2str}\")\n",
    "\n",
    "# Create a dictionary to map label names to their corresponding integer indices\n",
    "labels_str2int = {l: i for i, l in enumerate(labels_int2str)}\n",
    "\n",
    "# Cast the \"ner_tags\" column to a sequence of ClassLabel with the defined label names\n",
    "data_splits.cast_column(\"ner_tags\", Sequence(ClassLabel(names=labels_int2str)))\n",
    "\n",
    "# Print the dataset splits to verify the changes\n",
    "print(data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gaqtJZZFmDMf"
   },
   "outputs": [],
   "source": [
    "# initialize pretrained BERT tokenizer. This might take a while the first time it's run because the model needs to be downloaded.\n",
    "# Note: if you change the BERT model later, don't forget to also change this!!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-uv_urtjmQH2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'para_index': 0, 'title': 'Myersiohyla liliae', 'doc_id': 'Myersiohyla liliae-0', 'content': 'Myersiohyla liliae is a species of frogs in the family Hylidae. It is endemic to the Pacaraima Mountains in Guyana and known from the region of its type locality in the Kaieteur National Park and from Imbaimadai. The species is dedicated to the daughter of its describer, Lili Kok.', 'page_id': '28259031', 'id': 'Ud-DXIcB1INCf0UyAseC', 'tokens': ['Myersiohyla', 'liliae', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'Hylidae.', 'It', 'is', 'endemic', 'to', 'the', 'Pacaraima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kaieteur', 'National', 'Park', 'and', 'from', 'Imbaimadai.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describer,', 'Lili', 'Kok.'], 'ner_strings': ['B-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'ner_tags': [5, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]}\n",
      "{'para_index': 0, 'title': 'Hadingus', 'doc_id': 'Hadingus-0', 'content': \"Hadingus was one of the earliest legendary Danish kings according to Saxo Grammaticus' Gesta Danorum, where he has a detailed biography. Georges Dumézil and others have argued that Hadingus was partially modelled on the god Njörðr.\", 'page_id': '4283756', 'id': 'Gy_0WYcB1INCf0UycBhm', 'tokens': ['Hadingus', 'was', 'one', 'of', 'the', 'earliest', 'legendary', 'Danish', 'kings', 'according', 'to', 'Saxo', \"Grammaticus'\", 'Gesta', 'Danorum,', 'where', 'he', 'has', 'a', 'detailed', 'biography.', 'Georges', 'Dumézil', 'and', 'others', 'have', 'argued', 'that', 'Hadingus', 'was', 'partially', 'modelled', 'on', 'the', 'god', 'Njörðr.'], 'ner_strings': ['B-Mythological_king', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Mythological_king', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Mythological_king'], 'ner_tags': [2, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 12, 12, 12, 12, 12, 12, 2]}\n"
     ]
    }
   ],
   "source": [
    "# If you want you can look at some sample data items\n",
    "print(data_splits[\"train\"][8])\n",
    "print(data_splits[\"dev\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "utsg41nOizGz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'para_index': Value(dtype='int64', id=None), 'title': Value(dtype='string', id=None), 'doc_id': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None), 'page_id': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_strings': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "Original tokens: ['Myersiohyla', 'liliae', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'Hylidae.', 'It', 'is', 'endemic', 'to', 'the', 'Pacaraima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kaieteur', 'National', 'Park', 'and', 'from', 'Imbaimadai.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describer,', 'Lili', 'Kok.']\n",
      "NER labels: [5, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "Labels: ['B-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "BERT Tokenized:  ['[CLS]', 'Myers', '##io', '##hyl', '##a', 'l', '##ilia', '##e', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'H', '##yl', '##idae', '.', 'It', 'is', 'endemic', 'to', 'the', 'Pac', '##ara', '##ima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kai', '##ete', '##ur', 'National', 'Park', 'and', 'from', 'I', '##mba', '##ima', '##dai', '.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describe', '##r', ',', 'Lil', '##i', 'Ko', '##k', '.', '[SEP]']\n",
      "Vocab size: 28996\n",
      "Token IDs:  [101, 14311, 2660, 18873, 1161, 181, 26502, 1162, 1110, 170, 1530, 1104, 22025, 1107, 1103, 1266, 145, 7777, 5106, 119, 1135, 1110, 6850, 1106, 1103, 19430, 4626, 8628, 5249, 1107, 20345, 1105, 1227, 1121, 1103, 1805, 1104, 1157, 2076, 10157, 1107, 1103, 13354, 16618, 2149, 1305, 1670, 1105, 1121, 146, 10806, 8628, 14117, 119, 1109, 1530, 1110, 3256, 1106, 1103, 1797, 1104, 1157, 5594, 1197, 117, 14138, 1182, 19892, 1377, 119, 102]\n",
      "[None, 0, 0, 0, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 16, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 31, 31, 32, 33, 34, 35, 36, 36, 36, 36, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 46, 46, 47, 47, 48, 48, 48, None]\n"
     ]
    }
   ],
   "source": [
    "# This dataset is split into a train, validation and test set, and each token has a label.\n",
    "# Data from the dataset can generally be accessed like a Python dict.\n",
    "print(data_splits['train'].features)\n",
    "\n",
    "# Print the original sentence (which is whitespace tokenized).\n",
    "example_input_tokens = data_splits['train'][8]['tokens']\n",
    "print(f\"Original tokens: {example_input_tokens}\")\n",
    "\n",
    "# Print the labels of the sentence.\n",
    "example_ner_labels = data_splits['train'][8]['ner_tags']\n",
    "print(f\"NER labels: {example_ner_labels}\")\n",
    "\n",
    "# Map integer to string labels for the sentence\n",
    "example_mapped_labels = [labels_int2str[l] for l in example_ner_labels]\n",
    "print(f'Labels: {example_mapped_labels}')\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "example_tokenized = tokenizer(example_input_tokens, is_split_into_words=True)\n",
    "print('BERT Tokenized: ', example_tokenized.tokens())\n",
    "\n",
    "# Print the number of tokens in the vocabulary\n",
    "print(f'Vocab size: {tokenizer.vocab_size}')\n",
    "\n",
    "# # Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(example_tokenized.tokens()))\n",
    "\n",
    "# Of course, there are now way more tokens than labels! Fortunately the HF tokenizer\n",
    "# provides a function that will give us the mapping:\n",
    "print(example_tokenized.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZCTjD23gHKFB"
   },
   "outputs": [],
   "source": [
    "# We can write a function that uses that along with the original labels to get the new set of labels\n",
    "# for each BERT-tokenized token.\n",
    "# def labels_tokens_alignment(labels, word_ids):\n",
    "#     new_labels = []\n",
    "#     current_word = None\n",
    "#     for word_id in word_ids:\n",
    "#         if word_id != current_word:\n",
    "#             # Start of a new word!\n",
    "#             current_word = word_id\n",
    "#             label = -100 if word_id is None else labels[word_id]\n",
    "#             new_labels.append(label)\n",
    "#         elif word_id is None:\n",
    "#             # Special token\n",
    "#             new_labels.append(-100)\n",
    "#         else:\n",
    "#             # Same word as previous token\n",
    "#             label = labels[word_id]\n",
    "#             str_label = labels_int2str[label]\n",
    "#             if str_label[0] == 'B':\n",
    "#                 new_str_label = 'I' + str_label[1:]\n",
    "#                 label = labels_str2int[new_str_label]\n",
    "#             new_labels.append(label)\n",
    "\n",
    "#     return new_labels\n",
    "\n",
    "\n",
    "def labels_tokens_alignment(labels, word_ids):\n",
    "    new_labels = []  # Initialize a list to store the new labels\n",
    "    current_word = None  # Variable to keep track of the current word ID\n",
    "    for word_id in word_ids:  # Iterate over each word ID in the word_ids list\n",
    "        if word_id != current_word:  # Check if the word ID has changed\n",
    "            current_word = word_id  # Update the current word ID\n",
    "            # Append -100 if the word ID is None, otherwise append the corresponding label\n",
    "            new_labels.append(-100 if word_id is None else labels[word_id])\n",
    "        else:  # If the word ID is the same as the previous one\n",
    "            # Append -100 if the word ID is None, otherwise check if the label starts with 'B'\n",
    "            # If it does, change 'B' to 'I' and append the corresponding label, otherwise append the original label\n",
    "            new_labels.append(-100 if word_id is None else labels_str2int['I' + labels_int2str[labels[word_id]][1:]] if labels_int2str[labels[word_id]][0] == 'B' else labels[word_id])\n",
    "    return new_labels  # Return the list of new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zP8KavRk2z9j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned labels: [-100, 5, 0, 0, 0, 0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n",
      "Mapped aligned labels: ['_', 'B-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '_']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_aligned_labels = labels_tokens_alignment(example_ner_labels, example_tokenized.word_ids())\n",
    "print(f'Aligned labels: {tokenizer_aligned_labels}')\n",
    "print(f'Mapped aligned labels: {[labels_int2str[l] if l >= 0 else \"_\" for l in tokenizer_aligned_labels]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "m4cpmrYdHxbS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'Myers', '##io', '##hyl', '##a', 'l', '##ilia', '##e', 'is', 'a', 'species', 'of', 'frogs', 'in', 'the', 'family', 'H', '##yl', '##idae', '.', 'It', 'is', 'endemic', 'to', 'the', 'Pac', '##ara', '##ima', 'Mountains', 'in', 'Guyana', 'and', 'known', 'from', 'the', 'region', 'of', 'its', 'type', 'locality', 'in', 'the', 'Kai', '##ete', '##ur', 'National', 'Park', 'and', 'from', 'I', '##mba', '##ima', '##dai', '.', 'The', 'species', 'is', 'dedicated', 'to', 'the', 'daughter', 'of', 'its', 'describe', '##r', ',', 'Lil', '##i', 'Ko', '##k', '.', '[SEP]']\n",
      "Aligned labels: ['_', 'B-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'I-Aquatic_animal', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '_']\n"
     ]
    }
   ],
   "source": [
    "# Let's check the function on the example from before. The special tokens don't have labels,\n",
    "# so we'll just replace those with _\n",
    "aligned_labels = labels_tokens_alignment(example_ner_labels, example_tokenized.word_ids())\n",
    "print(f\"Tokens: {example_tokenized.tokens()}\")\n",
    "print(f\"Aligned labels: {[labels_int2str[l] if l >= 0 else '_' for l in aligned_labels]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "X2qrKgKe7E8N"
   },
   "outputs": [],
   "source": [
    "# Need to get the whole dataset into this format, so need to write a fn\n",
    "# we can apply efficiently across all examples using Dataset.map.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the input tokens with truncation and word splitting\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]  # Extract the NER tags from the examples\n",
    "    new_labels = []  # Initialize a list to store the new labels for all examples\n",
    "    for i, labels in enumerate(all_labels):  # Iterate over each set of labels\n",
    "        word_ids = tokenized_inputs.word_ids(i)  # Get the word IDs for the current example\n",
    "        # Align the labels with the tokens and append the result to new_labels\n",
    "        new_labels.append(labels_tokens_alignment(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels  # Add the new labels to the tokenized inputs\n",
    "    return tokenized_inputs  # Return the tokenized inputs with the new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GmY5nHOO65kV"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834d56a1c4ed40b580bf8840ea9ee1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d397349b4c8c41af8c78975f6492dbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab5619c003444ebb5f5432d59845043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can apply that fn to tokenize all the data\n",
    "tokenized_data_splits = data_splits.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=data_splits[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P9gLGKXq6YJO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "[-100, 9, 4, 4, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 9, 4, 4, 4, 4, 4, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n",
      "[-100, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n"
     ]
    }
   ],
   "source": [
    "# Testing batcher\n",
    "print(\"Examples:\")\n",
    "for i in range(2):\n",
    "    print(tokenized_data_splits[\"train\"][i][\"labels\"])\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_data_splits[\"train\"][i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5GMWXy2o-vu4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before: [9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "Labels after: ['B-Cretaceous_dinosaur', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Cretaceous_dinosaur', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Cretaceous_dinosaur': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.5,\n",
       " 'overall_f1': 0.6666666666666666,\n",
       " 'overall_accuracy': 0.9904761904761905}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation: we can use the seqeval library to handle calculating span-level precision, recall and F1\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "labels = data_splits[\"train\"][0][\"ner_tags\"]\n",
    "print(\"Labels before:\", labels)\n",
    "labels = [labels_int2str[i] for i in labels]\n",
    "print(\"Labels after:\", labels)\n",
    "\n",
    "# Make a small change and see how it impacts the score\n",
    "predictions = labels.copy()\n",
    "predictions[0] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQcF7uNCZ7qp"
   },
   "outputs": [],
   "source": [
    "# This code trains the model and evaluates it on test data. It should print\n",
    "# progress messages during training indicating loss, accuracy and training speed.\n",
    "# You will likely need to make changes to this code for it to work for token classification.\n",
    "# \n",
    "# TODO: change this\n",
    "def train(model,\n",
    "          train_dataset,\n",
    "          val_dataset,\n",
    "          num_epochs,\n",
    "          batch_size,\n",
    "          optimizer_cls,\n",
    "          lr,\n",
    "          weight_decay,\n",
    "          device,\n",
    "          collate_fn=None,\n",
    "          log_every=100):\n",
    "  # Set the model to training mode and move it to the specified device\n",
    "  model = model.train().to(device)\n",
    "  # Create a DataLoader for the training dataset\n",
    "  dataloader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "  # Initialize the optimizer based on the specified optimizer class\n",
    "  if optimizer_cls == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  elif optimizer_cls == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  elif optimizer_cls == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "  # Initialize lists to store training and validation metrics\n",
    "  train_loss_history = []\n",
    "  train_acc_history = []\n",
    "  val_loss_history = []\n",
    "  val_acc_history = []\n",
    "\n",
    "  # Define the loss function\n",
    "  lossfn = nn.NLLLoss()\n",
    "  for e in range(num_epochs):  # Loop over each epoch\n",
    "    model.train(True)  # Set the model to training mode\n",
    "    epoch_loss_history = []\n",
    "    epoch_acc_history = []\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Training batches\")):  # Loop over each batch\n",
    "      # Move the batch to the specified device\n",
    "      batch = {k:v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "      y = batch.pop('labels')  # Extract the labels from the batch\n",
    "      \n",
    "      logits = model(**batch)  # Forward pass\n",
    "        \n",
    "      # Apply log-softmax to logits before passing to NLLLoss\n",
    "      log_probs = torch.log_softmax(logits, dim=-1)\n",
    "      loss = lossfn(log_probs.view(-1, log_probs.size(-1)), y.view(-1))  # Compute the loss\n",
    "      \n",
    "      pred = logits.argmax(dim=-1)  # Get the predictions\n",
    "      acc = (pred == y).float().mean()  # Compute the accuracy\n",
    "\n",
    "      epoch_loss_history.append(loss.item())  # Append the loss to the epoch history\n",
    "      epoch_acc_history.append(acc.item())  # Append the accuracy to the epoch history\n",
    "\n",
    "      if (i % log_every == 0):  # Log the training progress every 'log_every' iterations\n",
    "        speed = 0 if i == 0 else log_every/(time.time()-start_time)\n",
    "        print(f'epoch: {e}\\t iter: {i}\\t train_loss: {np.mean(epoch_loss_history):.3e}\\t train_acc:{np.mean(epoch_acc_history):.3f}\\t speed:{speed:.3f} b/s')\n",
    "        start_time = time.time()\n",
    "      loss.backward()  # Backward pass\n",
    "      optimizer.step()  # Update the model parameters\n",
    "      optimizer.zero_grad()  # Zero the gradients\n",
    "      \n",
    "    # Evaluate the model on the validation dataset\n",
    "    val_loss, val_metrics, predictions = run_eval(model, val_dataset, batch_size, device, collate_fn=collate_fn)\n",
    "\n",
    "    val_acc = val_metrics['overall_accuracy']\n",
    "    val_p = val_metrics['overall_precision']\n",
    "    val_r = val_metrics['overall_recall']\n",
    "    val_f1 = val_metrics['overall_f1']\n",
    "\n",
    "    # Append the metrics to the history lists\n",
    "    train_loss_history.append(np.mean(epoch_loss_history))\n",
    "    train_acc_history.append(np.mean(epoch_acc_history))\n",
    "    val_loss_history.append(val_loss.item())\n",
    "    val_acc_history.append(val_acc)\n",
    "    print(f'epoch: {e}\\t train_loss: {train_loss_history[-1]:.3e}\\t train_accuracy:{train_acc_history[-1]:.3f}\\t val_loss: {val_loss_history[-1]:.3e}\\t val_acc:{val_acc_history[-1]:.3f}\\t val_p:{val_p:.3f}\\t val_r:{val_r:.3f}\\t val_f1:{val_f1:.3f}')\n",
    "\n",
    "  # Return the trained model and the training/validation metrics\n",
    "  return model, (train_loss_history, train_acc_history, val_loss_history, val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the un-used memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb0wjC-zTRz0"
   },
   "outputs": [],
   "source": [
    "# This code defines the token classification class using BERT.\n",
    "# The classifier is defined on top of the final layer of BERT.\n",
    "# The classifier has 1 hidden layer with 128 hidden nodes though we have found that\n",
    "# using a smaller number of hidden nodes does not make much difference,\n",
    "# \n",
    "# TODO: implement this\n",
    "class BertForTokenClassification(nn.Module):\n",
    "  def __init__(self, bert_pretrained_config_name, num_classes, freeze_bert=False, dropout_prob=0.1):\n",
    "    '''\n",
    "    BERT with a classification MLP\n",
    "    args:\n",
    "    - bert_pretrained_config_name (str): model name from huggingface hub\n",
    "    - num_classes (int): number of classes in the classification task\n",
    "    - freeze_bert (bool): [default False] If true gradients are not computed for\n",
    "                          BERT's parameters.\n",
    "    - dropout_prob (float): [default 0.1] probability of dropping each activation.\n",
    "    '''\n",
    "    super().__init__()\n",
    "    # Load the pre-trained BERT model from Huggingface hub\n",
    "    self.bert = BertModel.from_pretrained(bert_pretrained_config_name)\n",
    "    # Freeze BERT parameters if freeze_bert is True\n",
    "    self.bert.requires_grad_(not freeze_bert)\n",
    "    \n",
    "    # Define a dropout layer\n",
    "    self.dropout = nn.Dropout(dropout_prob)\n",
    "    # Define a classifier with a linear layer\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "      # nn.ReLU(),\n",
    "      # nn.Dropout(dropout_prob),\n",
    "      # nn.Linear(128, num_classes)\n",
    "    )\n",
    "    \n",
    "  def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "    # Pass inputs through BERT model\n",
    "    outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    # Get the last hidden state from BERT outputs\n",
    "    sequence_output = outputs.last_hidden_state\n",
    "    # Apply dropout to the sequence output\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "    # Pass the sequence output through the classifier to get logits\n",
    "    logits = self.classifier(sequence_output)\n",
    "    return logits  # Return the logits   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the un-used memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "w6NVHqYSYds-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 13\n",
      "Trainable parameters: 108320269\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t iter: 0\t train_loss: 2.771e+00\t train_acc:0.011\t speed:0.000 b/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 55/55 [00:30<00:00,  1.80it/s]\n",
      "Evaluation batches: 100%|██████████| 5/5 [00:01<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t train_loss: 3.692e-01\t train_accuracy:0.346\t val_loss: 2.187e-01\t val_acc:0.942\t val_p:0.354\t val_r:0.070\t val_f1:0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   2%|▏         | 1/55 [00:00<00:15,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t iter: 0\t train_loss: 2.356e-01\t train_acc:0.288\t speed:0.000 b/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 55/55 [00:31<00:00,  1.77it/s]\n",
      "Evaluation batches: 100%|██████████| 5/5 [00:01<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\t train_loss: 1.836e-01\t train_accuracy:0.340\t val_loss: 1.685e-01\t val_acc:0.944\t val_p:0.218\t val_r:0.287\t val_f1:0.248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   2%|▏         | 1/55 [00:00<00:17,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t iter: 0\t train_loss: 1.507e-01\t train_acc:0.233\t speed:0.000 b/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 55/55 [00:31<00:00,  1.76it/s]\n",
      "Evaluation batches: 100%|██████████| 5/5 [00:01<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\t train_loss: 1.043e-01\t train_accuracy:0.345\t val_loss: 1.607e-01\t val_acc:0.954\t val_p:0.364\t val_r:0.338\t val_f1:0.351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   2%|▏         | 1/55 [00:00<00:08,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\t iter: 0\t train_loss: 6.712e-02\t train_acc:0.425\t speed:0.000 b/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 55/55 [00:30<00:00,  1.79it/s]\n",
      "Evaluation batches: 100%|██████████| 5/5 [00:01<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\t train_loss: 6.217e-02\t train_accuracy:0.355\t val_loss: 1.573e-01\t val_acc:0.954\t val_p:0.403\t val_r:0.518\t val_f1:0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches:   2%|▏         | 1/55 [00:00<00:14,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\t iter: 0\t train_loss: 3.656e-02\t train_acc:0.340\t speed:0.000 b/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|██████████| 55/55 [00:31<00:00,  1.76it/s]\n",
      "Evaluation batches: 100%|██████████| 5/5 [00:01<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\t train_loss: 3.401e-02\t train_accuracy:0.357\t val_loss: 1.997e-01\t val_acc:0.953\t val_p:0.428\t val_r:0.500\t val_f1:0.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batches: 100%|██████████| 5/5 [00:00<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Loss: 1.997e-01\t Final Accuracy: 0.953\t dev_p:0.428\t dev_r:0.500\t dev_f1:0.461\n"
     ]
    }
   ],
   "source": [
    "# This is where fine-tuning of the classifier happens.\n",
    "# Here we are training with batch size 32 for 5 epochs.\n",
    "\n",
    "# At the end of each epoch, you also see validation loss and validation accuracy.\n",
    "# Change the device as described above if you will not be using a GPU\n",
    "\n",
    "# Set the random seed(s) for reproducability\n",
    "torch.random.manual_seed(8942764)\n",
    "torch.cuda.manual_seed(8942764)\n",
    "np.random.seed(8942764)\n",
    "\n",
    "# Make sure this is the same as you use for tokenization!\n",
    "bert_model = 'bert-base-cased'\n",
    "\n",
    "num_labels = len(labels_int2str)\n",
    "print(f\"Num labels: {num_labels}\")\n",
    "\n",
    "# conll hyperparams\n",
    "# multiply your learning rate by k when using batch size of kN\n",
    "lr = 4*2e-5 # 1e-3\n",
    "weight_decay = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "dropout_prob = 0.2\n",
    "freeze_bert = False\n",
    "\n",
    "bert_cls = BertForTokenClassification(bert_model, num_labels, dropout_prob=dropout_prob, freeze_bert=freeze_bert)\n",
    "\n",
    "print(f'Trainable parameters: {sum([p.numel() for p in bert_cls.parameters() if p.requires_grad])}\\n')\n",
    "\n",
    "# Flag for setting \"debug\" mode. Set debug to False for full training.\n",
    "debug = False\n",
    "\n",
    "# Sample a subset of the training data for faster iteration in debug mode\n",
    "subset_size = 1000\n",
    "subset_indices = torch.randperm(len(tokenized_data_splits['train']))[:subset_size]\n",
    "train_subset = Subset(tokenized_data_splits['train'], subset_indices)\n",
    "\n",
    "bert_cls, bert_cls_logs = train(bert_cls, tokenized_data_splits['train'] if not debug else train_subset, tokenized_data_splits['dev'],\n",
    "                                num_epochs=epochs, batch_size=batch_size, optimizer_cls='AdamW',\n",
    "                                lr=lr, weight_decay=weight_decay, device=device,\n",
    "                                collate_fn=data_collator, log_every=10 if debug else 100)\n",
    "\n",
    "# Final eval\n",
    "final_loss, final_metrics, eval_pred = run_eval(bert_cls, tokenized_data_splits['dev'], batch_size=32, device=device, collate_fn=data_collator)\n",
    "final_acc = final_metrics['overall_accuracy']\n",
    "final_p = final_metrics['overall_precision']\n",
    "final_r = final_metrics['overall_recall']\n",
    "final_f1 = final_metrics['overall_f1']\n",
    "print(f'\\nFinal Loss: {final_loss:.3e}\\t Final Accuracy: {final_acc:.3f}\\t dev_p:{final_p:.3f}\\t dev_r:{final_r:.3f}\\t dev_f1:{final_f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape (evaluation) 150\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction shape (evaluation)\", len(eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batches: 100%|██████████| 10/10 [00:01<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_metrics, test_pred = run_eval(bert_cls, tokenized_data_splits['test'], batch_size=32, device=device, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped aligned labels saved to test_predictions_bert.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file name for saving the mapped predictions\n",
    "output_file = \"test_predictions_bert.json\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Save the test predictions to the JSON file with indentation for readability\n",
    "    json.dump(test_pred, f, indent=4)\n",
    "\n",
    "# Print a message indicating that the mapped aligned labels have been saved\n",
    "print(f\"Mapped aligned labels saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP_Assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
